Concepts
Recursion

One way to think about recursion is to look at a map
Lets say I want to get to A.
But only one way to get to A is through B, C, & D.
This is true since only one road gets to A and the other locations
are stops on the way there.

Lets look at a representation of this:
B -> C -> D -> A

Recursion needs two cases: A base case and inductive case
A base case would be check if you are at A.
An inductive case would usually follow this base case.  It could possiblely
Check the adjacent location.

Say we have a function f(n) where n is the location.  The function would
recursively look to A.

Heres the code for this algorthm:
Assign n = <some location on the map>
Invoke f(n)
	Check if n = A print "Found A!" exit program //base case
	Assign n = n.next() //this assigns n to the next node
	return f(n) //recursive case
	
How would this run if we passed n = B;
Invoke f(B)
Check if its A <false>
Reassign n to n.next = C
Invoke f(C)
Check if its A <false>
Reassign n to n.next = D
Invoke f(D)
Check if its A <false>
Reassign n to n.next = A
Invoke f(A)
Check if its A <true>
print "Found A!"

Another way of look at this is to say the functions all inside one another
We are trying to solve for f(f(f(f(A))))
Where we want to go is "buried" in our initial function call

What's the benefit as supposed to iteration?

How would this be solved iteratively?
We would store B,C,D,A in an array
We go through the indexs of the array until we arrive at A and be done

So one could say that if a problem like this can easily mapped to an index
Recursion not really benefital to use since it seems more complicated.
But imagine the path was not so straight forward?
Imagine we had two branches per node like this
B -> C -> A
  -> D -> E
  	   -> F
  	   -> G
Iteration could still be used but it would be messy to program all the conditionals.
The power of recursion is that we can map our problem to something that cannot be
easily dealt with with indexing/iteration.  Iteration requires we move to the next index
and do the same stuff.  Recursion allows us solve a smaller chunk of the problem until we
arrive at the solution.  Different problems "map" better to a resursive algorthm.

How is memory handled with recursion when compared to iteration?

Useful terms
Program stack: Place in memory where function calls are stored
The main function is on the bottom.
Stack frame: Buffer memory that has data from the program stack such as
	Return address
	Input parameters
	Local variables
	Register Savings
Stack pointer - pointer that points to the top of the stack
(the most recent function called)

What happens when we call a function
1.Stack frame is pushed into stack (location of function)
The counter of call is 100 but function is on 104
2.Sub-routine instructions are executed.
Address from 2000-2020 are executed. 
3.Stack Frame is popped from the stack.
The 104 frame is taken off the stack
4.Now Program counter is holding the return address

Heap memory is what is stored while the program is running
Stack memory is memory that is temporarily stored when functions
are called.

Here is an example of how memory is stored:

package com.journaldev.test; 
public class Memory { 
 public static void main(String[] args) { // Line 1 
 int i=1; // Line 2 
 Object obj = new Object(); // Line 3 
 Memory mem = new Memory(); // Line 4 
 mem.foo(obj); // Line 5
  } // Line 9
 private void foo(Object param) { // Line 6
  String str = param.toString();  //// Line 7 
  System.out.println(str); } // Line 8
}

All Runtime classes are loaded into the Heap Space when the program is run.
Java Runtime creates Stack memory to be used by main() method thread when 
it is found at line 1. 
At line 2, a primitive local variable is created, which is 
stored in the Stack memory of main() method.
Since an Object is created at line 3, it’s created in 
Heap memory and the reference for it is stored in Stack memory. 
At line 4, a similar process occurs when a Memory object is created.
When foo() method is called at line 5, a block in the 
top of the Stack is created for it. Since Java is pass by value, 
a new reference to Object is created in the foo() stack block in line 6.
At line 7, a string is created, which goes in the 
String Pool in the Heap space, while a reference for 
it is created in the foo() stack space. 
At line 8, foo() method is terminated, and the memory 
block allocated for it in the Stack is freed.
Finally, at line 9, main() method terminates, and the 
Stack memory created for it is destroyed. 
Because the program ends at this line, Java Runtime 
frees all the memory and ends the execution of the program.

In some programming languages, the maximum size of the call stack is much 
less than the space available in the heap, and recursive algorithms 
tend to require more stack space than iterative algorithms. 
Consequently, these languages 
sometimes place a limit on the depth of recursion to avoid stack overflows;

These stack allocations can sometime designed to not handle recursion well,
since they assume you will will "get in and out" instead of constantly staying
in the function call stacking more and more calls.

More information about stack and heap memory can be found here:
https://www.geeksforgeeks.org/stack-vs-heap-memory-allocation/

Dynammic programming

This is what we call a divide and conquer approach to problem solving. DP also
employs what is called memoization.  This is storing already solved problems to reduce
redundant calculations.  The best example to look at this kind of problem is to model it
as a tree.  Take simple fib sequence where n = 4  so execution would stop at n - 3 or n - 4

                     f(n)
                  /        \
             f(n-1)        f(n-2)
           /       \     /        \
        f(n-2)    f(n-3) f(n-3)  f(n-4)
       /      \
   f(n-3)   f(n-4)    
        
If we execute everything without memoization we have 2^(tree depth) runtime.  So 2^n runtime.
This is very slow.  One way we can reduce runtime is to store already solved solutions.
We have 2 * f(n-2) function calls.  If we remove one of them by creating a memo of its solution we get 
a tree like this:

                     f(n)
                  /        \
             f(n-1)        f(n-2)
           /       \    
        f(n-2)    f(n-3) 
       /      \
   f(n-3)   f(n-4)
   
This turns the solution into 2n runtime.  Considerably faster.

As far as memory goes, when we store function calls the place in memory is freed up
after the call is completed.  So for instance when we get all the way down to f(n-1) we store the result
to the next function call up f(n -2).  After that the memory from f(n - 1) is freed up.  So the largest memory allocated
is when we get to the bottom ie f(f(f(f(n-3)))) so the memory usage is tree depth.  This is true for both
memoized and non-memoized versions of the calcuation.


Features

Typical implementation

Recursion is an easy implementation

For instance factoral function is:

f(n)
if n = 1 return n
return n * f(n-1)

Here are few variations to this pattern

return f(n)
This tells the program to call the result up the tree directly

if(f(n) <some conditions>)
This is not returning the function call directly up the tree.

var = f(n)
This is storing the function call that can be later compared.  Its not returning
the call up the tree.

We return the same function.

Memoization is usually employed using a hashtable or map.  This allows quick constant runtime
assuming collisions are managable.

//Hashtable implementation
// No need to mention the 
// Generic type twice 
Hashtable<Integer, String> ht1 = new Hashtable<>(); 
  
// Initialization of a Hashtable 
// using Generics 
Hashtable<Integer, String> ht2 = new Hashtable<Integer, String>(); 

//Hashmap implementation
import java.util.HashMap; // import the HashMap class
HashMap<String, String> capitalCities = new HashMap<String, String>();

Difference between the two:

Hashmap vs Hashtable
1. HashMap is non synchronized. It is not-thread safe and can’t be shared between many 
threads without proper synchronization code whereas Hashtable is synchronized. It is thread-safe and can be shared with many threads.
2. HashMap allows one null key and multiple null values whereas Hashtable doesn’t allow any null key or value.
3. HashMap is generally preferred over HashTable if thread synchronization is not needed

Why HashTable doesn’t allow null and HashMap does?
To successfully store and retrieve objects from a HashTable, the objects 
used as keys must implement the hashCode method and the equals method. 
Since null is not an object, it can’t implement these methods. 
HashMap is an advanced version and improvement on the Hashtable. HashMap was created later.